{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LLM Math Reasoning: Experiment Runner & Analyzer**\n",
    "\n",
    "This notebook serves two purposes:\n",
    "\n",
    "1.  **Run Experiments:** Execute the Python scripts to generate the JSON output files for each experiment.\n",
    "2.  **Analyze Results:** Load the generated JSON files, calculate final accuracies, and display sample responses to analyze model behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. **Setup**\n",
    "\n",
    "First, let's install the requirements and log in to Hugging Face. You only need to run this once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will need a Hugging Face token to access the Mistral model.\n",
    "# Run this cell and paste your token when prompted.\n",
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. **Run Experiments**\n",
    "\n",
    "The following cells will run all 7 experiments (with their variants). This will take a **very long time** (potentially several hours) and requires a powerful GPU.\n",
    "\n",
    "**Note:** The main script uses `argparse`, so we call it from the command line using `!python ...`. We must navigate to the root directory first (`cd ..`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Experiment 1: Direct Prompting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the most basic method and serves as our baseline. We give the model the math problem and ask it to provide the answer directly, without any examples or instructions to \"think step-by-step.\" \n",
    "\n",
    "We test two variants:\n",
    "* `v1`: Asks for a concise answer with no extra commentary.\n",
    "* `v2`: Asks for *only* the final numeric answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/run_experiment.py --experiment direct --variant 1 --sample_numbers 50\n",
    "!python scripts/run_experiment.py --experiment direct --variant 2 --sample_numbers 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Experiment 2: Zero-Shot CoT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method tests the \"Chain-of-Thought\" (CoT) concept. We don't provide any examples, but we add a simple phrase like \"Let's think step by step\" to the prompt. This encourages the model to generate a reasoning process *before* giving the final answer, which often improves accuracy on complex problems.\n",
    "\n",
    "* `v1`: Uses the classic \"Let's think step by step.\"\n",
    "* `v2`: Uses a slightly different prompt: \"Reason through it carefully and stepwise.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/run_experiment.py --experiment zero_shot --variant 1 --sample_numbers 50\n",
    "!python scripts/run_experiment.py --experiment zero_shot --variant 2 --sample_numbers 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Experiment 3: Few-Shot (Positive)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we provide the model with a few *correct* examples (shots) of questions and their step-by-step solutions before giving it the new problem. The idea is that the model learns the correct format and reasoning pattern from these examples (`in-context learning`).\n",
    "\n",
    "We test this with 2, 4, and 8 shots loaded from `data/prompts/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/run_experiment.py --experiment few_shot --shots 2 --sample_numbers 50\n",
    "!python scripts/run_experiment.py --experiment few_shot --shots 4 --sample_numbers 50\n",
    "!python scripts/run_experiment.py --experiment few_shot --shots 8 --sample_numbers 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Experiment 4: Few-Shot (Negative)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a counter-intuitive test. We provide the model with examples of questions that are solved *incorrectly*. The goal is to see if the model learns the *format* of reasoning (Q&A) while being smart enough to *ignore* the flawed logic, or if it gets confused and copies the mistakes.\n",
    "\n",
    "We test this with 2, 4, and 8 \"wrong\" shots loaded from `data/prompts/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/run_experiment.py --experiment wrong_shot --shots 2 --sample_numbers 50\n",
    "!python scripts/run_experiment.py --experiment wrong_shot --shots 4 --sample_numbers 50\n",
    "!python scripts/run_experiment.py --experiment wrong_shot --shots 8 --sample_numbers 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Experiment 5: Self-Consistency**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method uses a \"majority vote\" approach. We run the same prompt multiple times (K=3, K=4, K=5) with sampling enabled (`do_sample=True`), which produces different reasoning paths. We then extract the final answer from each path and choose the answer that appears most frequently.\n",
    "\n",
    "This helps to reduce the impact of a single flawed reasoning path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/run_experiment.py --experiment self_consistency --k_samples 3 --sample_numbers 50\n",
    "!python scripts/run_experiment.py --experiment self_consistency --k_samples 4 --sample_numbers 50\n",
    "!python scripts/run_experiment.py --experiment self_consistency --k_samples 5 --sample_numbers 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Experiment 6: Verbalized Confidence**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we ask the model to do two things for each of its K generated answers: \n",
    "1.  Solve the problem.\n",
    "2.  State a \"Confidence: X%\" score for its own answer.\n",
    "\n",
    "We then select the single answer that the model claims to be the *most confident* about. This tests the model's self-evaluation capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/run_experiment.py --experiment verbalized --k_samples 3 --sample_numbers 50\n",
    "!python scripts/run_experiment.py --experiment verbalized --k_samples 4 --sample_numbers 50\n",
    "!python scripts/run_experiment.py --experiment verbalized --k_samples 5 --sample_numbers 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Experiment 7: Subquestion Decomposition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the most complex method, also known as \"Least-to-Most\" prompting. \n",
    "\n",
    "1.  We first ask the model to break the main problem down into a list of simpler sub-questions.\n",
    "2.  Then, we feed each sub-question back to the model one by one, building a chain of reasoning by adding the previous answers to the context.\n",
    "3.  Finally, we ask for the final answer based on all the intermediate steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/run_experiment.py --experiment subquestion --sample_numbers 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. **Analyze Results**\n",
    "\n",
    "If you have already run the experiments (or downloaded the results), you can run the cells below to analyze the outputs.\n",
    "\n",
    "We will import the helper functions from our `src` module to calculate accuracy and print samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the src directory to the Python path to import our modules\n",
    "sys.path.append('../src')\n",
    "\n",
    "from llm_math_eval.evaluation import get_accuracy, print_sample_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = Path('../outputs')\n",
    "all_results = []\n",
    "\n",
    "# Define all expected result files\n",
    "result_files = {\n",
    "    'Direct Prompt (v1)': OUTPUT_DIR / 'direct_prompt_v1.json',\n",
    "    'Direct Prompt (v2)': OUTPUT_DIR / 'direct_prompt_v2.json',\n",
    "    'Zero-Shot CoT (v1)': OUTPUT_DIR / 'zero_shot_v1.json',\n",
    "    'Zero-Shot CoT (v2)': OUTPUT_DIR / 'zero_shot_v2.json',\n",
    "    'Few-Shot (2-shot)': OUTPUT_DIR / 'few_shot_2.json',\n",
    "    'Few-Shot (4-shot)': OUTPUT_DIR / 'few_shot_4.json',\n",
    "    'Few-Shot (8-shot)': OUTPUT_DIR / 'few_shot_8.json',\n",
    "    'Wrong-Shot (2-shot)': OUTPUT_DIR / 'wrong_shot_2.json',\n",
    "    'Wrong-Shot (4-shot)': OUTPUT_DIR / 'wrong_shot_4.json',\n",
    "    'Wrong-Shot (8-shot)': OUTPUT_DIR / 'wrong_shot_8.json',\n",
    "    'Self-Consistency (K=3)': OUTPUT_DIR / 'self_consistency_3.json',\n",
    "    'Self-Consistency (K=4)': OUTPUT_DIR / 'self_consistency_4.json',\n",
    "    'Self-Consistency (K=5)': OUTPUT_DIR / 'self_consistency_5.json',\n",
    "    'Verbalized Conf. (K=3)': OUTPUT_DIR / 'verbalized_confidence_3.json',\n",
    "    'Verbalized Conf. (K=4)': OUTPUT_DIR / 'verbalized_confidence_4.json',\n",
    "    'Verdbalized Conf. (K=5)': OUTPUT_DIR / 'verbalized_confidence_5.json',\n",
    "    'Subquestion Decomp.': OUTPUT_DIR / 'subquestion.json',\n",
    "}\n",
    "\n",
    "print(\"--- Accuracy Results ---\")\n",
    "for name, path in result_files.items():\n",
    "    if path.exists():\n",
    "        acc = get_accuracy(path)\n",
    "        all_results.append({'Method': name, 'Accuracy': acc})\n",
    "    else:\n",
    "        print(f\"File not found, skipping: {path}\")\n",
    "        all_results.append({'Method': name, 'Accuracy': None})\n",
    "\n",
    "# Display results in a clean table\n",
    "df_results = pd.DataFrame(all_results)\n",
    "df_results = df_results.dropna()\n",
    "df_results['Accuracy'] = pd.to_numeric(df_results['Accuracy'], errors='coerce')\n",
    "df_results = df_results.sort_values(by='Accuracy', ascending=False).reset_index(drop=True)\n",
    "\n",
    "display(df_results.style.format({'Accuracy': '{:.2%}'}).hide(axis='index'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sample Analysis**\n",
    "\n",
    "Let's look at some samples from the best-performing and worst-performing methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the name of the best and worst method from our DataFrame\n",
    "if not df_results.empty and pd.notna(df_results.iloc[0]['Accuracy']):\n",
    "    best_method_name = df_results.iloc[0]['Method']\n",
    "    worst_method_name = df_results.iloc[-1]['Method']\n",
    "\n",
    "    best_file = result_files[best_method_name]\n",
    "    worst_file = result_files[worst_method_name]\n",
    "\n",
    "    print(f\"\\n\\n--- Samples from BEST Method: {best_method_name} ---\")\n",
    "    print_sample_responses(best_file, num_samples=3)\n",
    "\n",
    "    print(f\"\\n\\n--- Samples from WORST Method: {worst_method_name} ---\")\n",
    "    print_sample_responses(worst_file, num_samples=3)\n",
    "else:\n",
    "    print(\"Could not determine best/worst methods. Please ensure result files exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also inspect any specific file\n",
    "print(\"\\n\\n--- Samples from Zero-Shot CoT (v1) ---\")\n",
    "if (OUTPUT_DIR / 'zero_shot_v1.json').exists():\n",
    "    print_sample_responses(OUTPUT_DIR / 'zero_shot_v1.json', num_samples=3)\n",
    "else:\n",
    "    print(\"File 'zero_shot_v1.json' not found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
